{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%cd /kaggle/input/mlxardata1/Test_Minist\n!ls","metadata":{"id":"M1o9OSKJ4bXB","outputId":"f196ae60-6876-4330-80a7-6a4c6518c9c1","execution":{"iopub.status.busy":"2023-03-25T05:20:21.464816Z","iopub.execute_input":"2023-03-25T05:20:21.467505Z","iopub.status.idle":"2023-03-25T05:20:22.608894Z","shell.execute_reply.started":"2023-03-25T05:20:21.467453Z","shell.execute_reply":"2023-03-25T05:20:22.607541Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/mlxardata1/Test_Minist\nconfigs  models  run.sh  test_imgs  tools  utils\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu116/torch1.13/index.html\n!pip install mmsegmentation\n!pip install timm\n!pip install datasets\n# !pip install comet-ml","metadata":{"execution":{"iopub.status.busy":"2023-03-25T05:20:22.610533Z","iopub.execute_input":"2023-03-25T05:20:22.610934Z","iopub.status.idle":"2023-03-25T05:21:36.535742Z","shell.execute_reply.started":"2023-03-25T05:20:22.610888Z","shell.execute_reply":"2023-03-25T05:21:36.534469Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Looking in links: https://download.openmmlab.com/mmcv/dist/cu116/torch1.13/index.html\nCollecting mmcv-full==1.7.0\n  Downloading https://download.openmmlab.com/mmcv/dist/cu116/torch1.13.0/mmcv_full-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (46.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from mmcv-full==1.7.0) (6.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from mmcv-full==1.7.0) (23.0)\nRequirement already satisfied: yapf in /opt/conda/lib/python3.7/site-packages (from mmcv-full==1.7.0) (0.32.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from mmcv-full==1.7.0) (1.21.6)\nCollecting addict\n  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\nRequirement already satisfied: opencv-python>=3 in /opt/conda/lib/python3.7/site-packages (from mmcv-full==1.7.0) (4.5.4.60)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from mmcv-full==1.7.0) (9.4.0)\nInstalling collected packages: addict, mmcv-full\nSuccessfully installed addict-2.4.0 mmcv-full-1.7.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting mmsegmentation\n  Downloading mmsegmentation-0.30.0-py3-none-any.whl (831 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.7/831.7 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from mmsegmentation) (23.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from mmsegmentation) (1.21.6)\nCollecting mmcls>=0.20.1\n  Downloading mmcls-0.25.0-py2.py3-none-any.whl (648 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m648.8/648.8 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: prettytable in /opt/conda/lib/python3.7/site-packages (from mmsegmentation) (0.7.2)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from mmsegmentation) (3.5.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmsegmentation) (1.4.4)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmsegmentation) (3.0.9)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmsegmentation) (9.4.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmsegmentation) (0.11.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmsegmentation) (2.8.2)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmsegmentation) (4.38.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->mmsegmentation) (4.4.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->mmsegmentation) (1.16.0)\nInstalling collected packages: mmcls, mmsegmentation\nSuccessfully installed mmcls-0.25.0 mmsegmentation-0.30.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: timm in /opt/conda/lib/python3.7/site-packages (0.6.12)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.7/site-packages (from timm) (0.12.1)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.14.0)\nRequirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.7/site-packages (from timm) (1.13.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from timm) (6.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.7->timm) (4.4.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (2.28.2)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (4.64.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (23.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (4.11.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (3.9.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (9.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (1.21.6)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub->timm) (3.11.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (1.26.14)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (3.4)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.1.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (23.0)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.12.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.28.2)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2023.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.3)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.11.4)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.64.1)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (5.0.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.6)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.4.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.8.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (22.2.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.9.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.11.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.7.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch \nimport albumentations as A\n\nfrom torch.nn.functional import one_hot\nfrom torch import nn\nfrom torch.utils.tensorboard import SummaryWriter\nimport torchvision\nfrom torchvision.utils import make_grid\nfrom torchvision.io import read_image\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom datetime import datetime\n\nimport json\nimport time","metadata":{"id":"VNhp0J0_HLWc","execution":{"iopub.status.busy":"2023-03-25T05:24:13.460001Z","iopub.execute_input":"2023-03-25T05:24:13.460675Z","iopub.status.idle":"2023-03-25T05:24:13.467222Z","shell.execute_reply.started":"2023-03-25T05:24:13.460630Z","shell.execute_reply":"2023-03-25T05:24:13.466048Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\";\ndevice","metadata":{"id":"fY8j6eXj4wKG","outputId":"b8cafcf2-b1ff-458d-8ced-b5fa6b5e11e1","execution":{"iopub.status.busy":"2023-03-25T05:24:13.831884Z","iopub.execute_input":"2023-03-25T05:24:13.832379Z","iopub.status.idle":"2023-03-25T05:24:13.843239Z","shell.execute_reply.started":"2023-03-25T05:24:13.832333Z","shell.execute_reply":"2023-03-25T05:24:13.841969Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"print(torch.version.cuda), print(torch.__version__)","metadata":{"id":"SdJwMyTmHP2f","outputId":"4b87ea7d-1406-430d-9bee-8b018216721c","execution":{"iopub.status.busy":"2023-03-25T05:24:14.229356Z","iopub.execute_input":"2023-03-25T05:24:14.229858Z","iopub.status.idle":"2023-03-25T05:24:14.238638Z","shell.execute_reply.started":"2023-03-25T05:24:14.229803Z","shell.execute_reply":"2023-03-25T05:24:14.237433Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"11.3\n1.13.0\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(None, None)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Getting Model","metadata":{"id":"dORFlX3z_EES"}},{"cell_type":"code","source":"# for k in dataloaderTest:\n#     print(k[3].shape)\n#     e1 = k[3]\n#     break\n# # Auxi \n# from utils.segformer import FCNHead\n# auxi_net = FCNHead(num_convs=1,\n#                         kernel_size=3,\n#                         concat_input=True,\n#                         in_channels=320,\n#                         num_classes=512,\n#                         norm_cfg=dict(type='SyncBN', requires_grad=True))\n# auxi_net = auxi_net.to(device)\n# auxi_out = auxi_net([a,b,c,d])\n# print(f\"Auxi Net: {auxi_out.shape}\") #torch.Size([2, 64, 64, 64]) torch.Size([2, 128, 32, 32]) torch.Size([2, 320, 16, 16]) torch.Size([2, 512, 8, 8]) outpuy->torch.Size([2, 512, 16, 16])\n# #v = a.max(1)[1]\n# print(\"Auxi Output \",auxi_out.shape)\n\n# ll = np.load('/kaggle/input/mlxardata1/Test_Minist/models/universal_cat2vec.npy', mmap_mode='r')\n# print(\"npy shape: \",ll.shape)\n# embeddInput = torch.tensor(ll,device=device)\n\n# embedd = torch.nn.Embedding(num_embeddings=ll.shape[0], embedding_dim=ll.shape[1], _weight = embeddInput)\n# e1 = e1.float()\n# e1 = torch.permute(e1,(0,3,1,2))\n# print(f\"e1 shape {e1.shape}\")\n# e1 = torch.nn.functional.interpolate(e1, size=(16,16), mode='bilinear', align_corners=True)\n# print(f\"After Resze e1 shape {e1.shape}\")\n\n# predictedLabelsIndex = torch.argmax(e1, dim=1)\n# print(e1.shape, predictedLabelsIndex.shape)\n \n# outEmbed = embedd(predictedLabelsIndex.to(device))\n# outEmbed.shape","metadata":{"id":"bfFdCM8j13wW","execution":{"iopub.status.busy":"2023-03-25T05:24:15.037012Z","iopub.execute_input":"2023-03-25T05:24:15.037488Z","iopub.status.idle":"2023-03-25T05:24:15.044715Z","shell.execute_reply.started":"2023-03-25T05:24:15.037445Z","shell.execute_reply":"2023-03-25T05:24:15.043653Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Mertic Class \nclass Metric(object):\n    \"\"\"Base class for all metrics.\n\n    From: https://github.com/pytorch/tnt/blob/master/torchnet/meter/meter.py\n    \"\"\"\n    def reset(self):\n        pass\n\n    def add(self):\n        pass\n\n    def value(self):\n        pass\nclass ConfusionMatrix(Metric):\n    \"\"\"Constructs a confusion matrix for a multi-class classification problems.\n\n    Does not support multi-label, multi-class problems.\n\n    Keyword arguments:\n    - num_classes (int): number of classes in the classification problem.\n    - normalized (boolean, optional): Determines whether or not the confusion\n    matrix is normalized or not. Default: False.\n\n    Modified from: https://github.com/pytorch/tnt/blob/master/torchnet/meter/confusionmeter.py\n    \"\"\"\n\n    def __init__(self, num_classes, normalized=False):\n        super().__init__()\n\n        self.conf = np.ndarray((num_classes, num_classes), dtype=np.int64)\n        self.normalized = normalized\n        self.num_classes = num_classes\n        self.reset()\n\n    def reset(self):\n        self.conf.fill(0)\n\n    def add(self, predicted, target):\n        \"\"\"Computes the confusion matrix\n\n        The shape of the confusion matrix is K x K, where K is the number\n        of classes.\n\n        Keyword arguments:\n        - predicted (Tensor or numpy.ndarray): Can be an N x K tensor/array of\n        predicted scores obtained from the model for N examples and K classes,\n        or an N-tensor/array of integer values between 0 and K-1.\n        - target (Tensor or numpy.ndarray): Can be an N x K tensor/array of\n        ground-truth classes for N examples and K classes, or an N-tensor/array\n        of integer values between 0 and K-1.\n\n        \"\"\"\n        # If target and/or predicted are tensors, convert them to numpy arrays\n        if torch.is_tensor(predicted):\n            predicted = predicted.cpu().numpy()\n        if torch.is_tensor(target):\n            target = target.cpu().numpy()\n\n        assert predicted.shape[0] == target.shape[0], \\\n            'number of targets and predicted outputs do not match'\n\n        if np.ndim(predicted) != 1:\n            assert predicted.shape[1] == self.num_classes, \\\n                'number of predictions does not match size of confusion matrix'\n            predicted = np.argmax(predicted, 1)\n        else:\n            assert (predicted.max() < self.num_classes) and (predicted.min() >= 0), \\\n                'predicted values are not between 0 and k-1'\n\n        if np.ndim(target) != 1:\n            assert target.shape[1] == self.num_classes, \\\n                'Onehot target does not match size of confusion matrix'\n            assert (target >= 0).all() and (target <= 1).all(), \\\n                'in one-hot encoding, target values should be 0 or 1'\n            assert (target.sum(1) == 1).all(), \\\n                'multi-label setting is not supported'\n            target = np.argmax(target, 1)\n        else:\n            assert (target.max() < self.num_classes) and (target.min() >= 0), \\\n                'target values are not between 0 and k-1'\n\n        # hack for bincounting 2 arrays together\n        x = predicted + self.num_classes * target\n        bincount_2d = np.bincount(\n            x.astype(np.int64), minlength=self.num_classes**2)\n        assert bincount_2d.size == self.num_classes**2\n        conf = bincount_2d.reshape((self.num_classes, self.num_classes))\n\n        self.conf += conf\n\n    def value(self):\n        \"\"\"\n        Returns:\n            Confustion matrix of K rows and K columns, where rows corresponds\n            to ground-truth targets and columns corresponds to predicted\n            targets.\n        \"\"\"\n        if self.normalized:\n            conf = self.conf.astype(np.float32)\n            return conf / conf.sum(1).clip(min=1e-12)[:, None]\n        else:\n            return self.conf\nclass IoU(Metric):\n    \"\"\"Computes the intersection over union (IoU) per class and corresponding\n    mean (mIoU).\n\n    Intersection over union (IoU) is a common evaluation metric for semantic\n    segmentation. The predictions are first accumulated in a confusion matrix\n    and the IoU is computed from it as follows:\n\n        IoU = true_positive / (true_positive + false_positive + false_negative).\n\n    Keyword arguments:\n    - num_classes (int): number of classes in the classification problem\n    - normalized (boolean, optional): Determines whether or not the confusion\n    matrix is normalized or not. Default: False.\n    - ignore_index (int or iterable, optional): Index of the classes to ignore\n    when computing the IoU. Can be an int, or any iterable of ints.\n    \"\"\"\n\n    def __init__(self, num_classes, normalized=False, ignore_index=None):\n        super().__init__()\n        self.conf_metric = ConfusionMatrix(num_classes, normalized)\n\n        if ignore_index is None:\n            self.ignore_index = None\n        elif isinstance(ignore_index, int):\n            self.ignore_index = (ignore_index,)\n        else:\n            try:\n                self.ignore_index = tuple(ignore_index)\n            except TypeError:\n                raise ValueError(\"'ignore_index' must be an int or iterable\")\n\n    def reset(self):\n        self.conf_metric.reset()\n\n    def add(self, predicted, target):\n        \"\"\"Adds the predicted and target pair to the IoU metric.\n\n        Keyword arguments:\n        - predicted (Tensor): Can be a (N, K, H, W) tensor of\n        predicted scores obtained from the model for N examples and K classes,\n        or (N, H, W) tensor of integer values between 0 and K-1.\n        - target (Tensor): Can be a (N, K, H, W) tensor of\n        target scores for N examples and K classes, or (N, H, W) tensor of\n        integer values between 0 and K-1.\n\n        \"\"\"\n        # Dimensions check\n        assert predicted.size(0) == target.size(0), \\\n            'number of targets and predicted outputs do not match'\n        assert predicted.dim() == 3 or predicted.dim() == 4, \\\n            \"predictions must be of dimension (N, H, W) or (N, K, H, W)\"\n        assert target.dim() == 3 or target.dim() == 4, \\\n            \"targets must be of dimension (N, H, W) or (N, K, H, W)\"\n\n        # If the tensor is in categorical format convert it to integer format\n        if predicted.dim() == 4:\n            _, predicted = predicted.max(1)\n        if target.dim() == 4:\n            _, target = target.max(1)\n\n        self.conf_metric.add(predicted.view(-1), target.view(-1))\n\n    def value(self):\n        \"\"\"Computes the IoU and mean IoU.\n\n        The mean computation ignores NaN elements of the IoU array.\n\n        Returns:\n            Tuple: (IoU, mIoU). The first output is the per class IoU,\n            for K classes it's numpy.ndarray with K elements. The second output,\n            is the mean IoU.\n        \"\"\"\n        conf_matrix = self.conf_metric.value()\n        if self.ignore_index is not None:\n            conf_matrix[:, self.ignore_index] = 0\n            conf_matrix[self.ignore_index, :] = 0\n        true_positive = np.diag(conf_matrix)\n        false_positive = np.sum(conf_matrix, 0) - true_positive\n        false_negative = np.sum(conf_matrix, 1) - true_positive\n\n        # Just in case we get a division by 0, ignore/hide the error\n        with np.errstate(divide='ignore', invalid='ignore'):\n            iou = true_positive / (true_positive + false_positive + false_negative)\n\n        return iou, np.nanmean(iou)\nclass DiceAndPixelAcc(object):\n    r\"\"\"Calculate common metrics in semantic segmentation to evalueate model preformance.\n    Supported metrics: Pixel accuracy, Dice Coeff, precision score and recall score.\n    \n    Pixel accuracy measures how many pixels in a image are predicted correctly.\n    Dice Coeff is a measure function to measure similarity over 2 sets, which is usually used to\n    calculate the similarity of two samples. Dice equals to f1 score in semantic segmentation tasks.\n    \n    It should be noted that Dice Coeff and Intersection over Union are highly related, so you need \n    NOT calculate these metrics both, the other can be calcultaed directly when knowing one of them.\n    Precision describes the purity of our positive detections relative to the ground truth. Of all\n    the objects that we predicted in a given image, precision score describes how many of those objects\n    actually had a matching ground truth annotation.\n    Recall describes the completeness of our positive predictions relative to the ground truth. Of\n    all the objected annotated in our ground truth, recall score describes how many true positive instances\n    we have captured in semantic segmentation.\n    Args:\n        eps: float, a value added to the denominator for numerical stability.\n            Default: 1e-5\n        average: bool. Default: ``True``\n            When set to ``True``, average Dice Coeff, precision and recall are\n            returned. Otherwise Dice Coeff, precision and recall of each class\n            will be returned as a numpy array.\n        ignore_background: bool. Default: ``True``\n            When set to ``True``, the class will not calculate related metrics on\n            background pixels. When the segmentation of background pixels is not\n            important, set this value to ``True``.\n        activation: [None, 'none', 'softmax' (default), 'sigmoid', '0-1']\n            This parameter determines what kind of activation function that will be\n            applied on model output.\n    Input:\n        y_true: :math:`(N, H, W)`, torch tensor, where we use int value between (0, num_class - 1)\n        to denote every class, where ``0`` denotes background class.\n        y_pred: :math:`(N, C, H, W)`, torch tensor.\n    Examples::\n        >>> metric_calculator = SegmentationMetrics(average=True, ignore_background=True)\n        >>> pixel_accuracy, dice, precision, recall = metric_calculator(y_true, y_pred)\n    \"\"\"\n    def __init__(self, eps=1e-5, average=True, ignore_background=True, activation='0-1'):\n        self.eps = eps\n        self.average = average\n        self.ignore = ignore_background\n        self.activation = activation\n\n    @staticmethod\n    def _one_hot(gt, pred, class_num):\n        # transform sparse mask into one-hot mask\n        # shape: (B, H, W) -> (B, C, H, W)\n        input_shape = tuple(gt.shape)  # (N, H, W, ...)\n        new_shape = (input_shape[0], class_num) + input_shape[1:]\n        one_hot = torch.zeros(new_shape).to(pred.device, dtype=torch.float)\n        target = one_hot.scatter_(1, gt.unsqueeze(1).long().data, 1.0)\n        return target\n\n    @staticmethod\n    def _get_class_data(gt_onehot, pred, class_num):\n        # perform calculation on a batch\n        # for precise result in a single image, plz set batch size to 1\n        matrix = np.zeros((3, class_num))\n\n        # calculate tp, fp, fn per class\n        for i in range(class_num):\n            # pred shape: (N, H, W)\n            class_pred = pred[:, i, :, :]\n            # gt shape: (N, H, W), binary array where 0 denotes negative and 1 denotes positive\n            class_gt = gt_onehot[:, i, :, :]\n\n            pred_flat = class_pred.contiguous().view(-1, )  # shape: (N * H * W, )\n            gt_flat = class_gt.contiguous().view(-1, )  # shape: (N * H * W, )\n\n            tp = torch.sum(gt_flat * pred_flat)\n            fp = torch.sum(pred_flat) - tp\n            fn = torch.sum(gt_flat) - tp\n\n            matrix[:, i] = tp.item(), fp.item(), fn.item()\n\n        return matrix\n\n    def _calculate_multi_metrics(self, gt, pred, class_num):\n        # calculate metrics in multi-class segmentation\n        matrix = self._get_class_data(gt, pred, class_num)\n        if self.ignore:\n            matrix = matrix[:, 1:]\n\n        # tp = np.sum(matrix[0, :])\n        # fp = np.sum(matrix[1, :])\n        # fn = np.sum(matrix[2, :])\n\n        pixel_acc = (np.sum(matrix[0, :]) + self.eps) / (np.sum(matrix[0, :]) + np.sum(matrix[1, :]))\n        dice = (2 * matrix[0] + self.eps) / (2 * matrix[0] + matrix[1] + matrix[2] + self.eps)\n        precision = (matrix[0] + self.eps) / (matrix[0] + matrix[1] + self.eps)\n        recall = (matrix[0] + self.eps) / (matrix[0] + matrix[2] + self.eps)\n\n        if self.average:\n            dice = np.average(dice)\n            precision = np.average(precision)\n            recall = np.average(recall)\n\n        return pixel_acc, dice, precision, recall\n\n    def __call__(self, y_true, y_pred):\n        class_num = y_pred.size(1)\n\n        if self.activation in [None, 'none']:\n            activation_fn = lambda x: x\n            activated_pred = activation_fn(y_pred)\n        elif self.activation == \"sigmoid\":\n            activation_fn = nn.Sigmoid()\n            activated_pred = activation_fn(y_pred)\n        elif self.activation == \"softmax\":\n            activation_fn = nn.Softmax(dim=1)\n            activated_pred = activation_fn(y_pred)\n        elif self.activation == \"0-1\":\n            pred_argmax = torch.argmax(y_pred, dim=1)\n            activated_pred = self._one_hot(pred_argmax, y_pred, class_num)\n        else:\n            raise NotImplementedError(\"Not a supported activation!\")\n\n        gt_onehot = self._one_hot(y_true, y_pred, class_num)\n        pixel_acc, dice, precision, recall = self._calculate_multi_metrics(gt_onehot, activated_pred, class_num)\n        return pixel_acc, dice, precision, recall","metadata":{"execution":{"iopub.status.busy":"2023-03-25T05:24:15.624739Z","iopub.execute_input":"2023-03-25T05:24:15.625220Z","iopub.status.idle":"2023-03-25T05:24:15.687659Z","shell.execute_reply.started":"2023-03-25T05:24:15.625174Z","shell.execute_reply":"2023-03-25T05:24:15.686497Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset,DataLoader\nimport glob\nimport os\n\nfrom PIL import Image\nimport cv2\n\nfrom datasets import Dataset as dt\nfrom datasets import load_metric\n\nfrom datasets import DatasetDict\n#metric = load_metric(\"mean_iou\")","metadata":{"id":"nCtIcmEQOqYo","execution":{"iopub.status.busy":"2023-03-25T05:24:15.955559Z","iopub.execute_input":"2023-03-25T05:24:15.956000Z","iopub.status.idle":"2023-03-25T05:24:15.965492Z","shell.execute_reply.started":"2023-03-25T05:24:15.955955Z","shell.execute_reply":"2023-03-25T05:24:15.964484Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Dataloader Class","metadata":{"id":"l37yf-N8OXr0"}},{"cell_type":"code","source":"def check(a, b):\n  firstTerm, seconTerm, thirdTerm = b[0], b[1], b[2]\n\n  for index, i in enumerate(a):\n    if(i[0]==firstTerm and i[1]==seconTerm and i[2]==thirdTerm):\n      return index\n  return False","metadata":{"id":"IwtnJlX04FgS","execution":{"iopub.status.busy":"2023-03-25T05:24:16.916139Z","iopub.execute_input":"2023-03-25T05:24:16.916618Z","iopub.status.idle":"2023-03-25T05:24:16.924557Z","shell.execute_reply.started":"2023-03-25T05:24:16.916574Z","shell.execute_reply":"2023-03-25T05:24:16.923445Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"uniqueColors = [\n[  0,   0,   0],\n[  0,   0, 170],\n[  0,   0, 255],\n[  0,  85, 255],\n[  0, 170, 255],\n[  0, 255, 255],\n[ 85, 255, 170],\n[170,   0,   0],\n[170, 255,  85],\n[255,   0,   0],\n[255,  85,   0],\n[255, 170,   0],\n[255, 255,   0]\n]\nSizeToCrop = 256","metadata":{"id":"AyUNut_WaQgY","execution":{"iopub.status.busy":"2023-03-25T05:24:17.234128Z","iopub.execute_input":"2023-03-25T05:24:17.234510Z","iopub.status.idle":"2023-03-25T05:24:17.244283Z","shell.execute_reply.started":"2023-03-25T05:24:17.234477Z","shell.execute_reply":"2023-03-25T05:24:17.243087Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class CMPDataset(torch.utils.data.Dataset):\n    id2label = {\n        0: \"unknown\",\n        1: \"background\",\n        2: \"facade\", \n        3: \"window\", \n        4: \"door\", \n        5: \"cornice\", \n        6: \"sill\", \n        7: \"balcony\", \n        8: \"blind\",  \n        9: \"pillar\",\n        10: \"deco\", \n        11: \"molding\",\n        12: \"shop\"\n    }\n    \n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.img_paths = []\n        self.lbl_paths = []\n        self.label2id = {lbl: id for id, lbl in self.id2label.items()}\n\n        # Iterate over the files in the root directory\n        for file in sorted(os.listdir(root_dir)):\n            # Check if the file is a JPG image\n            if file.endswith(\".jpg\"):\n                self.img_paths.append(os.path.join(root_dir, file))\n            # Check if the file is an XML file\n            elif file.endswith(\".png\"):\n                self.lbl_paths.append(os.path.join(root_dir, file))\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        num_labels = 13\n        img_path = self.img_paths[idx]\n        lbl_path = self.lbl_paths[idx]\n        \n        #Reading Image and changing to RGB\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        originalImage = img\n        originalImage = cv2.resize(originalImage, (SizeToCrop,SizeToCrop))\n        \n        #Reading Mask and changing to RGB\n        maskcv2 = cv2.imread(lbl_path)\n        maskcv2 = cv2.cvtColor(maskcv2, cv2.COLOR_BGR2RGB)\n        originalMask = maskcv2\n        originalMask = cv2.resize(originalMask, (SizeToCrop,SizeToCrop))\n\n        # Transformation Before Masklabelling and OneHotEncoded.\n        if self.transform:\n            transformed = self.transform(image=img, mask=maskcv2)\n            img = transformed[\"image\"]\n            maskcv2 = transformed[\"mask\"]\n\n        w, h, c = maskcv2.shape\n        labelMask = np.zeros((w,h),dtype=int)\n        for i in range(0,maskcv2.shape[0]):\n            for j in range(0,maskcv2.shape[1]):\n                res = check(uniqueColors ,maskcv2[i, j, :])\n                if res:\n                    labelMask[i][j] = res\n\n        OneHotEncoded = one_hot(torch.tensor(labelMask), num_labels)\n        return img, labelMask, OneHotEncoded, maskcv2, originalImage, originalMask","metadata":{"execution":{"iopub.status.busy":"2023-03-25T05:24:17.813897Z","iopub.execute_input":"2023-03-25T05:24:17.814856Z","iopub.status.idle":"2023-03-25T05:24:17.828922Z","shell.execute_reply.started":"2023-03-25T05:24:17.814814Z","shell.execute_reply":"2023-03-25T05:24:17.827617Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# dataTrain = CMPDataset(\"/kaggle/input/datasetmlxar/CMP_facade_DB_base/base\")\n# dataTest = CMPDataset(\"/kaggle/input/datasetmlxar/CMP_facade_DB_base/test\")\ntrain_transform = A.Compose(\n    [\n        A.Resize(SizeToCrop, SizeToCrop),\n        A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5),\n        A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.5),\n        A.RandomBrightnessContrast(brightness_limit=0.4, contrast_limit=0.3, p=0.5),\n        A.RandomRotate90(p=0.4),\n        A.Transpose(p=0.5), # Change x-axis to y and Y-axis to X.\n        A.AdvancedBlur(blur_limit=(3, 7), p=0.5)\n    ]\n)\ndataTrain = CMPDataset(\"/kaggle/input/datasetmlxarr/CMP_facade_DB_base/base\", transform=train_transform,)\n\nval_transform = A.Compose(\n    [A.Resize(SizeToCrop, SizeToCrop)]\n)\ndataTest = CMPDataset(\"/kaggle/input/datasetmlxarr/CMP_facade_DB_base/test\", transform=val_transform,)\n\n","metadata":{"id":"UzOIpkWTmUsV","execution":{"iopub.status.busy":"2023-03-25T05:24:18.698420Z","iopub.execute_input":"2023-03-25T05:24:18.699428Z","iopub.status.idle":"2023-03-25T05:24:18.811122Z","shell.execute_reply.started":"2023-03-25T05:24:18.699372Z","shell.execute_reply":"2023-03-25T05:24:18.810076Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"dataTrain.__len__(), dataTest.__len__()","metadata":{"execution":{"iopub.status.busy":"2023-03-25T05:24:19.330532Z","iopub.execute_input":"2023-03-25T05:24:19.330929Z","iopub.status.idle":"2023-03-25T05:24:19.338281Z","shell.execute_reply.started":"2023-03-25T05:24:19.330890Z","shell.execute_reply":"2023-03-25T05:24:19.337185Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"(542, 64)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Dataloader","metadata":{"id":"rW1VeUaY6fVG"}},{"cell_type":"code","source":"dataloaderTrain = DataLoader(dataTrain, batch_size=4, shuffle=True)\ndataloaderTest = DataLoader(dataTest, batch_size=4, shuffle=True)","metadata":{"id":"ujVVcrs-2U5L","execution":{"iopub.status.busy":"2023-03-25T05:24:20.860370Z","iopub.execute_input":"2023-03-25T05:24:20.861108Z","iopub.status.idle":"2023-03-25T05:24:20.866660Z","shell.execute_reply.started":"2023-03-25T05:24:20.861062Z","shell.execute_reply":"2023-03-25T05:24:20.865380Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# for a in dataloaderTest:\n#     print(a[0].shape, a[1].shape, a[3].shape)\n#     e1 = a[3]\n#     break","metadata":{"execution":{"iopub.status.busy":"2023-03-25T05:24:21.355367Z","iopub.execute_input":"2023-03-25T05:24:21.356294Z","iopub.status.idle":"2023-03-25T05:24:21.361241Z","shell.execute_reply.started":"2023-03-25T05:24:21.356239Z","shell.execute_reply":"2023-03-25T05:24:21.360102Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Training Loop ","metadata":{"id":"yS2Nw53QwRXR"}},{"cell_type":"code","source":"from utils.segformer import SegFormer\n\n# SegFormer Head\nfrom utils.decode_heads import SegFormerHead\n\n# Auxi \nfrom utils.segformer import FCNHead","metadata":{"id":"_wW2PBbjycy9","outputId":"0c2d7a56-1384-4414-da82-d71b20f9923a","execution":{"iopub.status.busy":"2023-03-25T05:24:21.942783Z","iopub.execute_input":"2023-03-25T05:24:21.943281Z","iopub.status.idle":"2023-03-25T05:24:24.789434Z","shell.execute_reply.started":"2023-03-25T05:24:21.943241Z","shell.execute_reply":"2023-03-25T05:24:24.788214Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/mmcv/__init__.py:21: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n  'On January 1, 2023, MMCV will release v2.0.0, in which it will remove '\n","output_type":"stream"}]},{"cell_type":"code","source":"dist_url = 'tcp://127.0.0.1:6769'\ndist_url = dist_url[:-2] + str(os.getpid() % 100).zfill(2)\n\nnum_gpus = torch.cuda.device_count()\ntorch.distributed.init_process_group(backend=\"nccl\", init_method=dist_url, world_size=num_gpus, rank=0)","metadata":{"id":"8_1TVgOM6eGY","execution":{"iopub.status.busy":"2023-03-25T05:24:24.792070Z","iopub.execute_input":"2023-03-25T05:24:24.792429Z","iopub.status.idle":"2023-03-25T05:24:24.811825Z","shell.execute_reply.started":"2023-03-25T05:24:24.792383Z","shell.execute_reply":"2023-03-25T05:24:24.810874Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"class MyModel(nn.Module):\n  def __init__(self):\n    super(MyModel, self).__init__();\n\n    self.encoder = SegFormer(\"/kaggle/input/mlxardata1/Test_Minist/models/custom_new_name_2.pth\")\n\n    self.decoder = SegFormerHead(num_classes=13,\n                          in_channels=[64, 128, 320, 512],\n                          channels=128,\n                          in_index=[0,1,2,3],\n                          feature_strides=[4, 8, 16, 32],\n                          #decoder_params=dict(embed_dim=768),\n                          dropout_ratio=0.1,\n                          norm_cfg=dict(type='SyncBN', requires_grad=True),\n                          align_corners=False)\n    \n\n    self.auxiNet = auxi_net = FCNHead(num_convs=1,\n                        kernel_size=3,\n                        concat_input=True,\n                        in_channels=320,\n                        num_classes=512, # 512 Embediing Size\n                        norm_cfg=dict(type='SyncBN', requires_grad=True))\n    self.sig = torch.nn.Sigmoid()\n    \n  def forward(self, x):\n\n    w, h = x.shape[3], x.shape[2]\n    outEncoder = self.encoder(x)\n    outDecoder = self.decoder(outEncoder)\n    high_out = torch.nn.functional.interpolate(outDecoder, size=(h,w), mode='bilinear', align_corners=True)\n    high_out = self.sig(high_out)\n    \n    # Auxi Net\n    auxiOutput = self.auxiNet(outEncoder) # Input -> (torch.Size([2, 64, 64, 64]) torch.Size([2, 128, 32, 32]) torch.Size([2, 320, 16, 16]) torch.Size([2, 512, 8, 8])) \n    # auxiOutput output->torch.Size([2, 512, 16, 16])\n    \n    return high_out, auxiOutput","metadata":{"id":"WzXo3Ah_2gMH","execution":{"iopub.status.busy":"2023-03-25T05:24:28.772790Z","iopub.execute_input":"2023-03-25T05:24:28.773208Z","iopub.status.idle":"2023-03-25T05:24:28.784590Z","shell.execute_reply.started":"2023-03-25T05:24:28.773169Z","shell.execute_reply":"2023-03-25T05:24:28.783480Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"model = MyModel()\nmodel = model.to(device)\n#outs = model(torch.randn(2,3,256,256).to(device))","metadata":{"id":"_xR_NJz_5rsV","outputId":"c2421f39-f989-4342-965b-937fc28a454f","execution":{"iopub.status.busy":"2023-03-25T05:24:30.823387Z","iopub.execute_input":"2023-03-25T05:24:30.824139Z","iopub.status.idle":"2023-03-25T05:24:42.038975Z","shell.execute_reply.started":"2023-03-25T05:24:30.824091Z","shell.execute_reply":"2023-03-25T05:24:42.037860Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"2023-03-25 05:24:33,934 - mmseg - INFO - load checkpoint from local path: /kaggle/input/mlxardata1/Test_Minist/models/custom_new_name_2.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"class CustomLoss(nn.Module):\n    def __init__(self):\n        super(CustomLoss,self).__init__();\n        self.lossBCE = torch.nn.BCELoss()\n        self.lossL1 = torch.nn.L1Loss()\n    \n    def forward(self,outputsModel, OneHotEncodedLabel, y_pred, y_truth):\n        l1 = self.lossL1(y_pred, y_truth)\n        l2 = self.lossBCE(outputsModel, OneHotEncodedLabel)\n        TotalLoss = l1 + l2\n        return TotalLoss","metadata":{"execution":{"iopub.status.busy":"2023-03-25T05:26:50.485125Z","iopub.execute_input":"2023-03-25T05:26:50.485546Z","iopub.status.idle":"2023-03-25T05:26:50.495316Z","shell.execute_reply.started":"2023-03-25T05:26:50.485511Z","shell.execute_reply":"2023-03-25T05:26:50.494270Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"groundEmbeddings = np.load('/kaggle/input/mlxardata1/Test_Minist/models/universal_cat2vec.npy', mmap_mode='r')\nembeddInput = torch.tensor(groundEmbeddings, device=device)\n\nembedd = torch.nn.Embedding(num_embeddings=groundEmbeddings.shape[0], embedding_dim=groundEmbeddings.shape[1], _weight = embeddInput)\n\ndef preprocessForEmbedding(mask):\n    mask = mask.float()\n    mask = torch.permute(mask,(0,3,1,2))\n    maskResize = nn.functional.interpolate(mask, size=(16,16), mode='bilinear', align_corners=True)\n    \n    # argmax before converting to ClassIndex\n    predictedLabelsIndex = torch.argmax(maskResize, dim=1)\n    predictedLabelsIndex = predictedLabelsIndex.to(device)\n    groundEmbedded = embedd(predictedLabelsIndex)\n    groundEmbedded = torch.permute(groundEmbedded, (0,3,1,2))\n    return groundEmbedded # Output -> torch.Size([B, 512, 16, 16])","metadata":{"execution":{"iopub.status.busy":"2023-03-25T05:26:52.676788Z","iopub.execute_input":"2023-03-25T05:26:52.677187Z","iopub.status.idle":"2023-03-25T05:26:52.717139Z","shell.execute_reply.started":"2023-03-25T05:26:52.677149Z","shell.execute_reply":"2023-03-25T05:26:52.716115Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"epochs = 50\nmodelLoss = CustomLoss()\noptim = torch.optim.Adam(model.parameters())\n\nlossTrainPerEpoch = []\nlossTestPerEpoch = []\nmeanIOUPerEpoch = []\npixelAccPerEpoch = []\ndiceCoffPerEpoch = []","metadata":{"id":"E7l3-Igm6AWt","execution":{"iopub.status.busy":"2023-03-25T05:27:03.945790Z","iopub.execute_input":"2023-03-25T05:27:03.946199Z","iopub.status.idle":"2023-03-25T05:27:03.958615Z","shell.execute_reply.started":"2023-03-25T05:27:03.946162Z","shell.execute_reply":"2023-03-25T05:27:03.957519Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Metrics initialisation from metrics.py\n#from metrics import DiceAndPixelAcc, IoU","metadata":{"execution":{"iopub.status.busy":"2023-03-25T05:27:04.908491Z","iopub.execute_input":"2023-03-25T05:27:04.909319Z","iopub.status.idle":"2023-03-25T05:27:04.913691Z","shell.execute_reply.started":"2023-03-25T05:27:04.909277Z","shell.execute_reply":"2023-03-25T05:27:04.912564Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"instIOU = IoU(13)\ninstDicePixel = DiceAndPixelAcc(average=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T05:27:05.645196Z","iopub.execute_input":"2023-03-25T05:27:05.645575Z","iopub.status.idle":"2023-03-25T05:27:05.650743Z","shell.execute_reply.started":"2023-03-25T05:27:05.645541Z","shell.execute_reply":"2023-03-25T05:27:05.649560Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2023-03-25T05:27:06.405104Z","iopub.execute_input":"2023-03-25T05:27:06.405817Z","iopub.status.idle":"2023-03-25T05:27:06.412570Z","shell.execute_reply.started":"2023-03-25T05:27:06.405779Z","shell.execute_reply":"2023-03-25T05:27:06.411365Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"def IndexesToImage(img, uniqueColors):\n    innerList = []\n    imgList = []\n    for batchNumber, imgIndex in enumerate(img):\n        h,w = imgIndex.shape[1], imgIndex.shape[0]\n        innerList = []\n        for i in range(0,h):\n            for j in range(0,w):\n                innerList.append(uniqueColors[imgIndex[i][j]])\n        innerToNumpy = np.array(innerList).reshape(w,h,-1)\n        innerToNumpy = np.expand_dims(innerToNumpy, axis=0)\n        innerToTorch = torch.from_numpy(innerToNumpy)\n        imgList.append(innerToTorch)\n    \n    batchofTensors = torch.concat(imgList, axis=0)\n    return batchofTensors","metadata":{"execution":{"iopub.status.busy":"2023-03-25T05:27:10.054379Z","iopub.execute_input":"2023-03-25T05:27:10.055336Z","iopub.status.idle":"2023-03-25T05:27:10.064319Z","shell.execute_reply.started":"2023-03-25T05:27:10.055279Z","shell.execute_reply":"2023-03-25T05:27:10.063119Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# for indx,(img, groundLabel, OneHotEncodedLabel, mask) in enumerate(dataloaderTrain):\n#     print(img.shape, groundLabel.shape, OneHotEncodedLabel.shape, mask.shape)\n#     break\n# #torch.Size([4, 256, 256, 3]) torch.Size([4, 256, 256]) torch.Size([4, 256, 256, 13]) torch.Size([4, 256, 256, 3])","metadata":{"execution":{"iopub.status.busy":"2023-03-25T05:27:11.420992Z","iopub.execute_input":"2023-03-25T05:27:11.421775Z","iopub.status.idle":"2023-03-25T05:27:11.426583Z","shell.execute_reply.started":"2023-03-25T05:27:11.421735Z","shell.execute_reply":"2023-03-25T05:27:11.425451Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"for epoch in range(0,epochs):\n    print(f\"==================================================Epoch no: {epoch}========================================================\")\n    start_time = datetime.now()\n    lossTrain = 0\n    lossTest = 0\n    iou = 0\n    piAcc = 0\n    diCoff = 0\n    for indx,(img, groundLabel, OneHotEncodedLabel, mask,originalImage, originalMask) in enumerate(dataloaderTrain):\n\n        img = torch.tensor(img, device=device, dtype=torch.float32)\n        img = torch.permute(img, (0,3,1,2))\n        OneHotEncodedLabel = OneHotEncodedLabel.to(device)\n        OneHotEncodedLabel = OneHotEncodedLabel.type(torch.float32)\n        OneHotEncodedLabel = torch.permute(OneHotEncodedLabel, (0,3,1,2))\n\n        outputsModel = model(img)\n        optim.zero_grad()\n\n        postmasking = preprocessForEmbedding(mask)\n        loss = modelLoss(outputsModel[0], OneHotEncodedLabel, outputsModel[1], postmasking)\n        lossTrain+=loss.item()\n        loss.backward()\n        optim.step()\n        \n    lossTrainPerEpoch.append(lossTrain/(indx+1))\n    print(f\"Average Training Loss after Epoch Number {epoch} : {lossTrain/(indx+1)}\")\n    \n    # Model Evaluation\n    for indx,(img, groundLabel, OneHotEncodedLabel, mask, originalImageTest, originalMaskTest) in enumerate(dataloaderTest):\n        model.eval()\n\n        with torch.no_grad():\n            img = torch.tensor(img, device=device, dtype=torch.float32)\n            img = torch.permute(img, (0,3,1,2))\n            OneHotEncodedLabel = OneHotEncodedLabel.to(device)\n            OneHotEncodedLabel = OneHotEncodedLabel.type(torch.float32)\n            OneHotEncodedLabel = torch.permute(OneHotEncodedLabel, (0,3,1,2))\n            testOutputs = model(img)\n\n            # Model Loss\n            processMasking = preprocessForEmbedding(mask)\n            evalLoss = modelLoss(testOutputs[0], OneHotEncodedLabel, testOutputs[1], processMasking)\n            lossTest += evalLoss.item()\n\n            predictedLabelsIndex = torch.argmax(testOutputs[0], dim=1)\n\n            # Calculating Pixel Accuracy and Dice Coefficent\n            pixelAcc, diceCoff,_, _ = instDicePixel( groundLabel.to(device), testOutputs[0].to(device)) \n            instIOU.add(predictedLabelsIndex, groundLabel)\n            _, meanIOU = instIOU.value()\n            iou+=meanIOU\n            piAcc+=pixelAcc\n            diCoff+=diceCoff\n            \n    print(f\"Average Test Loss after Epoch Number {epoch} : {lossTest/(indx+1)}\")\n    print(f\"Pixel Accuracy after Epoch Number {epoch} : {piAcc/(indx+1)}\")\n    print(f\"Dice Coff after Epoch Number {epoch} : {diCoff/(indx+1)}\")\n\n    meanIOUPerEpoch.append(iou/len(dataloaderTest))\n    lossTestPerEpoch.append(lossTest/len(dataloaderTest))\n    pixelAccPerEpoch.append(piAcc/len(dataloaderTest))\n    diceCoffPerEpoch.append(diceCoff/len(dataloaderTest))\n\n    # Converting predictedLabelsIndex to RGB for Visualisation purpose\n    predictedLabels = IndexesToImage(predictedLabelsIndex, uniqueColors) # output -> Torch Tensor (B,3,256,256)\n\n    mask = torch.permute(mask,(0,3,1,2))\n    # Visualising Last Epoch: Images || Predicted Label || Groundlabel\n    mask = mask.to(device)\n    predictedLabels = predictedLabels.to(device)\n    predictedLabels = torch.permute(predictedLabels, (0,3,1,2))  \n\n    if epoch%7 == 0:\n        PATH = f'/kaggle/working/demo_model_Epoch{epoch}.pt'\n        torch.save(model.state_dict(), PATH)\n    end_time = datetime.now()\n    time_difference = (end_time - start_time).total_seconds() * 10**3\n    print(f\"Execution time of epoch {epoch} no : \", time_difference, \"ms\")\n    print(\"==========================================================================================================\")","metadata":{"id":"q38IBiT5dyGo","outputId":"4cc43f48-9ec0-4a4b-821b-23219fe29e13","execution":{"iopub.status.busy":"2023-03-25T05:36:28.343143Z","iopub.execute_input":"2023-03-25T05:36:28.343670Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"==================================================Epoch no: 0========================================================\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  # This is added back by InteractiveShellApp.init_path()\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss after Epoch Number 0 : 0.22606024639133146\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","output_type":"stream"},{"name":"stdout","text":"Average Test Loss after Epoch Number 0 : 0.16760160401463509\nPixel Accuracy after Epoch Number 0 : 0.5669572353744508\nDice Coff after Epoch Number 0 : 0.29200340018338616\nExecution time of epoch 0 no :  658583.697 ms\n==========================================================================================================\n==================================================Epoch no: 1========================================================\n","output_type":"stream"}]},{"cell_type":"code","source":"# Writing Final Results into Text File\nfinalDict = {}\nfinalDict['finalTrainLoss'] = lossTrain/len(dataloaderTrain)\nfinalDict[\"finalTestLoss\"] = lossTest/len(dataloaderTest)\nfinalDict[\"pixelAcc\"] = piAcc/len(dataloaderTest)\nfinalDict[\"IOU\"] = iou/len(dataloaderTest)\nfinalDict[\"diCoff\"] = diCoff/len(dataloaderTest)\n\nwith open(\"FinalResults.json\", \"w\") as outfile:\n    json.dump(finalDict, outfile)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting details\nplotDict = {}\nplotDict['finalTrainLoss'] = lossTrainPerEpoch\nplotDict[\"finalTestLoss\"] = lossTestPerEpoch\nplotDict[\"pixelAcc\"] = pixelAccPerEpoch\nplotDict[\"IOU\"] = meanIOUPerEpoch\nplotDict[\"diCoff\"] = diceCoffPerEpoch\nplotDict[\"epochs\"] = epochs\n\nwith open(\"plotData.json\", \"w\") as outfile:\n    json.dump(plotDict, outfile)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving Final batch images\ndef show_images(images, cols = 1, titles = None):\n    \"\"\"Display a list of images in a single figure with matplotlib.\n    \n    Parameters\n    ---------\n    images: List of np.arrays compatible with plt.imshow.\n    \n    cols (Default = 1): Number of columns in figure (number of rows is \n                        set to np.ceil(n_images/float(cols))).\n    \n    titles: List of titles corresponding to each image. Must have\n            the same length as titles.\n    \"\"\"\n    assert((titles is None)or (len(images) == len(titles)))\n    n_images = len(images)\n    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n    fig = plt.figure()\n    for n, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(cols,cols, n + 1)\n        if image.ndim == 2:\n            plt.gray()\n        print(image.shape)\n        plt.imshow(image)\n        a.set_title(title)\n    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n    plt.savefig(\"/kaggle/working/model.png\", bbox_inches='tight')\n    plt.show()\n# Inserting Images into list\nimagesList = []\nbatch = originalImageTest.shape[0] - 1\nprint(type(originalImage))\nimgSave = originalImageTest.detach().cpu().numpy()\nmaskSave = originalMaskTest.detach().cpu().numpy()\nprint(\"Before  Predicted permute: \",predictedLabelsSave.shape)\npredictedLabels = torch.permute(predictedLabels, (0,3,2,1))\npredictedLabelsSave = predictedLabels.detach().cpu().numpy()\nnamesOfImages = []\nprint(\"After Predicted permute: \",predictedLabelsSave.shape)\nfor i in range(0,batch):\n    newImg = imgSave[i]\n    h,w,c = newImg.shape[1], newImg.shape[0], newImg.shape[2]\n    newImg = newImg.reshape(w,h,c)\n    \n    newMask = maskSave[i]\n    h,w,c = newMask.shape[1], newMask.shape[0], newMask.shape[2]\n    newMask = newMask.reshape(w,h,c)\n    \n    NewpredictedLabels = predictedLabelsSave[i]\n    print(\"NEw Preedicted Labels: \",NewpredictedLabels.shape)\n    h,w,c = NewpredictedLabels.shape[1], NewpredictedLabels.shape[0], NewpredictedLabels.shape[2]\n    NewpredictedLabels = NewpredictedLabels.reshape(w,h,c)\n    \n    imagesList.append(newImg)\n    namesOfImages.append(f\"RGB Image: ({i+1})\")\n    imagesList.append(newMask)\n    namesOfImages.append(f\"Original Mask Image: ({i+1})\")\n    imagesList.append(NewpredictedLabels)\n    namesOfImages.append(f\"Predicted Image: ({i+1})\")\n\nshow_images(imagesList, int(3),namesOfImages)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model(img_test.unsqueeze(0).cuda()).deatch().cpu().clone().numpy()\n# which means that you are going to:\n# deatch --> cut computational graph\n# cpu --> allocate tensor in RAM\n# clone --> clone the tensor not to modify the output in-place\n# numpy --> port tensor to numpy","metadata":{"execution":{"iopub.status.busy":"2023-03-20T12:39:04.196632Z","iopub.execute_input":"2023-03-20T12:39:04.197784Z","iopub.status.idle":"2023-03-20T12:39:04.205327Z","shell.execute_reply.started":"2023-03-20T12:39:04.197747Z","shell.execute_reply":"2023-03-20T12:39:04.204248Z"},"trusted":true},"execution_count":29,"outputs":[]}]}